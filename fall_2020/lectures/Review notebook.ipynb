{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Probability\n",
    "- Confidence intervals and inequalities\n",
    "- Distances and Feature Spaces\n",
    "- Linear Algebra and Dimensionality reduction (SVD)\n",
    "- Applications of SVD\n",
    "- Clustering using maximum likelihood\n",
    "- Markov chains and node important measures (PAGERANK!)\n",
    "- Gradient Descent and SGD\n",
    "- Classification and model evaluation and selection (Bias/Variance tradeoff)\n",
    "- Relational model and Relational Algebra\n",
    "- Basics of Network Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** Consider a connected, weighted and undirected graph $G(V,E)$ with $V$ being the set of nodes and $E$ the set of edges.\n",
    "Let the weights of the edges be given by $w(x,y)$, for the undirected edge $(x,y)$. Prove that the stationary distribution \n",
    "of a random walk on this graph, where the transition probabilities \n",
    "$M(x,y)$ are given by:\n",
    "\n",
    "$$\n",
    "M(x,y) = \\frac{w(x,y)}{\\sum_{y\\in N(x)}w(x,y)},\n",
    "$$\n",
    "where $N(x)$ are the neighbors of $x$ in $G$,\n",
    "is:\n",
    "\n",
    "$$\n",
    "\\pi(x) = \\frac{\\sum_{y\\in N(x)}w(x,y)}{2\\sum_{(x,y)\\in E}w(x,y)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to show that $\\pi M = \\pi$ -- again I assume that $\\pi$ is a row vector.\n",
    "\n",
    "Let's compute $\\pi M = \\mathbf{v}$\n",
    "\n",
    "$\\pi = [\\frac{\\sum_{y\\in N(1)}w(1,y)}{2\\sum_{(x,y)\\in E}w(x,y)}, \\frac{\\sum_{y\\in N(2)}w(2,y)}{2\\sum_{(x,y)\\in E}w(x,y)},\\ldots , \\frac{\\sum_{y\\in N(n)}w(n,y)}{2\\sum_{(x,y)\\in E}w(x,y)}] $\n",
    "\n",
    "$\\pi = \\frac{1}{2\\sum_{(x,y)\\in E}w(x,y)}[\\sum_{y\\in N(1)}w(1,y), \\sum_{y\\in N(2)}w(2,y),\\ldots ,\\sum_{y\\in N(n)}w(n,y) ] $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{v}(i) = \\pi(1)M(1,i)+\\pi(2)M(2,i)+\\ldots +\\pi(n)M(n,i) = \\sum_{j=1}^n\\pi(j)M(j,i)=\\sum_{j=1}^n \\frac{\\sum_{i\\in N(j)}w(j,i)}{2\\sum_{(x,y)\\in E}w(x,y)} \\frac{w(j,i)}{\\sum_{i\\in N(j)}w(j,i)}$\n",
    "\n",
    "$= \\sum_{j=1}^n \\frac{}{2\\sum_{(x,y)\\in E}w(x,y)} \\frac{w(j,i)}{} = \\frac{\\sum_{j=1}^n w(j,i)}{2\\sum_{(x,y)\\in E}w(x,y)}= \\frac{\\sum_{j=\\in N(i)}w(j,i)}{2\\sum_{(x,y)\\in E}w(x,y)} =\\pi(i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2** Consider transition matrix $M$ such that $M(x,y)=M(y,x)$ for every pair of states $x,y$.\n",
    "If $M$ corresponds to an ergodic Markov Chain (i.e., connected and aperiodic) show that the stationary distribution\n",
    "of this Markov chain is:\n",
    "\n",
    "$$\n",
    "\\pi(x) = \\frac{1}{n},\n",
    "$$\n",
    "where $n$ is the number of states in the Markov chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pi:$ is the stationary probability distribution that is a column vector.\n",
    "\n",
    "$M\\pi = \\pi$\n",
    "\n",
    "if $\\pi$ is a row vector:\n",
    "\n",
    "$\\pi M = \\pi$\n",
    "\n",
    "In order to show that $\\pi = [1/n, 1/n,\\ldots , 1/n]$ is a stationary probability distribution, it is enough to show that $\\pi M = \\pi$ or in other words\n",
    "\n",
    "$$\n",
    "[\\frac{1}{n}, \\frac{1}{n},\\ldots , \\frac{1}{n}] M = [\\frac{1}{n}, \\frac{1}{n},\\ldots , \\frac{1}{n}].\n",
    "$$\n",
    "\n",
    "Let's consider the $i$-the entry of $\\pi$ to be $\\pi(i)$. Let's assume that \n",
    "\n",
    "$$\n",
    "[\\frac{1}{n}, \\frac{1}{n},\\ldots , \\frac{1}{n}] M = \\pi.\n",
    "$$\n",
    "\n",
    "It is enough to show that $\\pi(i)=\\frac{1}{n}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pi(i) = \\frac{1}{n}M(1,i)+\\frac{1}{n}M(2,i)+\\ldots + \\frac{1}{n} M(n,i) = \\frac{1}{n}\\sum_{j=1}^nM(j,i)=\\frac{1}{n}$\n",
    "\n",
    "This is because $M$ is a transistion matrix so $\\sum_jM(i,j)=1$, but in this case since $M$ is symmetric it is also true that for any $i$ $\\sum_{j=1}^nM(j,i)=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3** Assume a binary classification problem, where every data instance can belong to\n",
    "one of two possible classes: class A and class B.\n",
    "\n",
    "Assume a meta-classifier that classifies an instance as follows: it asks n independent classifiers to\n",
    "classify the instance. If the majority of the independent classifiers classify the instance as class A, so\n",
    "does the meta-classifier. Otherwise, the meta-classifier classifies the instance as class B. If each one of\n",
    "the independent classifiers makes a classification error with probability p, what is the probability of\n",
    "error of the meta-classifier? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ be a random variable that corresponds to the number of classifiers that make a mistake.\n",
    "\n",
    "$$\n",
    "Pr(X>\\frac{n}{2}) = Pr(X=\\frac{n}{2}+1)+Pr(X = \\frac{n}{2}+2)+\\ldots + Pr(X = n) = \\sum_{i=1}^{\\frac{n}{2}}{n\\choose n/2+i}p^{n/2+i}(1-p)^{n/2-i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Pr(X = k)= c(n,k)p^k(1-p)^{n-k} = {n\\choose k}p^k(1-p)^{n-k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4** Assume a binary classification problem, where every data instance can belong to one of two possible classes: class A and class B.\n",
    "\n",
    "Assume another meta-classifier that classifies an instance as class A, if there exists at least one independent classifier that classifies it as A. Otherwise, the meta-classifier classifies the instance as class\n",
    "B. What is the probability of error of the meta-classifier given that each independent classifier has\n",
    "probability of error $p$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two independent cases where the meta-classifier can make a mistake\n",
    "\n",
    "(a): When the object is B but the meta-classifier classifies it as A\n",
    "\n",
    "(b): When the object is A but the meta-classifier classifies it as B\n",
    "\n",
    "X : random variable that corresponds to whether the meta-classifier makes an error.\n",
    "\n",
    "$Pr(X) = Pr(a) + Pr (b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Pr(a) = Pr(\\text{At least one classifier made a mistake}) = 1-(1-p)^n$\n",
    "\n",
    "$Pr(b) = Pr(\\text{All classifiers made a mistake}) = p^n$\n",
    "\n",
    "$Pr(X) = 1-(1-p)^n + p^n$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
