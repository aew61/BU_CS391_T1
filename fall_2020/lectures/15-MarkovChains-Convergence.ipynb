{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Chains Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "internals": {},
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import networkx as nx\n",
    "from IPython.display import Image\n",
    "from IPython.display import display_html\n",
    "from IPython.display import display\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "from IPython.display import HTML\n",
    "import slideUtilities as sl\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "internals": {},
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       " .container.slides .celltoolbar, .container.slides .hide-in-slideshow {\n",
       "    display: None ! important;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    " .container.slides .celltoolbar, .container.slides .hide-in-slideshow {\n",
    "    display: None ! important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {},
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "%Set up useful MathJax (Latex) macros.\n",
    "%See http://docs.mathjax.org/en/latest/tex.html#defining-tex-macros\n",
    "%These are for use in the slideshow\n",
    "$\\newcommand{\\mat}[1]{\\left[\\begin{array}#1\\end{array}\\right]}$\n",
    "$\\newcommand{\\vx}{{\\mathbf x}}$\n",
    "$\\newcommand{\\hx}{\\hat{\\mathbf x}}$\n",
    "$\\newcommand{\\vbt}{{\\mathbf\\beta}}$\n",
    "$\\newcommand{\\vy}{{\\mathbf y}}$\n",
    "$\\newcommand{\\vz}{{\\mathbf z}}$\n",
    "$\\newcommand{\\R}{{\\mathbb{R}}}$\n",
    "$\\newcommand{\\vu}{{\\mathbf u}}$\n",
    "$\\newcommand{\\vv}{{\\mathbf v}}$\n",
    "$\\newcommand{\\vw}{{\\mathbf w}}$\n",
    "$\\newcommand{\\col}{{\\operatorname{Col}}}$\n",
    "$\\newcommand{\\nul}{{\\operatorname{Nul}}}$\n",
    "$\\newcommand{\\vb}{{\\mathbf b}}$\n",
    "$\\newcommand{\\va}{{\\mathbf a}}$\n",
    "$\\newcommand{\\ve}{{\\mathbf e}}$\n",
    "$\\newcommand{\\setb}{{\\mathcal{B}}}$\n",
    "$\\newcommand{\\rank}{{\\operatorname{rank}}}$\n",
    "$\\newcommand{\\vp}{{\\mathbf p}}$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "internals": {
     "slide_helper": "subslide_end"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\\newcommand{\\mat}[1]{\\left[\\begin{array}#1\\end{array}\\right]}\n",
    "\\newcommand{\\vx}{{\\mathbf x}}\n",
    "\\newcommand{\\hx}{\\hat{\\mathbf x}}\n",
    "\\newcommand{\\vbt}{{\\mathbf\\beta}}\n",
    "\\newcommand{\\vy}{{\\mathbf y}}\n",
    "\\newcommand{\\vz}{{\\mathbf z}}\n",
    "\\newcommand{\\vb}{{\\mathbf b}}\n",
    "\\newcommand{\\vu}{{\\mathbf u}}\n",
    "\\newcommand{\\vv}{{\\mathbf v}}\n",
    "\\newcommand{\\vw}{{\\mathbf w}}\n",
    "\\newcommand{\\va}{{\\mathbf a}}\n",
    "\\newcommand{\\ve}{{\\mathbf e}}\n",
    "\\newcommand{\\vp}{{\\mathbf p}}\n",
    "\\newcommand{\\R}{{\\mathbb{R}}}\n",
    "\\newcommand{\\col}{{\\operatorname{Col}}}\n",
    "\\newcommand{\\nul}{{\\operatorname{Nul}}}\n",
    "\\newcommand{\\rank}{{\\operatorname{rank}}}\n",
    "\\newcommand{\\setb}{{\\mathcal{B}}}\n",
    "\\renewcommand{\\Pr}{\\mathbb{P}}\n",
    "\\newcommand{\\E}{\\mathbb{E\\ }}\n",
    "\\DeclarePairedDelimiter{\\norm}{||}{||_1}\n",
    "\\DeclarePairedDelimiter{\\n}{||}{||_2}\n",
    "\\DeclarePairedDelimiterX{\\inp}[2]{\\langle}{\\rangle}{#1, #2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How long does it take for a random work on a graph to converge?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $M$ be the transition matrix of an ergodic (connected and aperiodic) Markov Chain.  What do we know about $M$?\n",
    "\n",
    "\n",
    "- $\\sum_{j}M(i,j)=1$ for every $i$\n",
    "\n",
    "- $\\pi = \\pi M \\rightarrow$ that $M$ has an eigenvalue $1$ and we showed that this is indeed the *largest* eigenvalue. \n",
    "\n",
    "\n",
    "What we are going to show is: the number of iterations of the power method in order to converge depends on the\n",
    "*second* largest eigenvalue, denote it by $\\lambda_2$ for which we know that $\\lambda_2<1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We assume $G = (V, E)$ is an undirected, unweighted\n",
    "and connected graph. For simplicity, we also assume that $G$ is $d$-regular (all nodes have degree $d$). \n",
    "\n",
    "Recall that $M = D^{−1}A$\n",
    "is the normalized adjacency matrix which will be transition matrix of the random walk on $G$.\n",
    " Since $M$ is a symmetric matrix, $\\pi = (1/n,\\ldots, 1/n)$ is the stationary distribution.\n",
    " \n",
    " \n",
    " Can you parse tha paragraph above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Lemma** Let $M$ be any symmetric transition matrix. Then for any probability vector $\\mathbf{x}$ and $\\pi = (1/n,\\ldots ,1/n)$ we have that\n",
    "\n",
    "$$\n",
    "||\\mathbf{x}M^t-\\pi||_2\\leq\\sqrt{n}\\lambda_2^t,\n",
    "$$\n",
    "where $\\lambda_2$ is the second largest eigenvalue of $M$ in absolute value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Observation** If $\\mathbf{e_1},\\ldots,\\mathbf{e_n}$ are eigenvectors of $M$ \n",
    "with corresponding eigenvalues $\\lambda_1,\\ldots,\\lambda_n$, then \n",
    "$\\mathbf{e_1},\\ldots,\\mathbf{e_n}$\n",
    "are also eigenvectors of $M^t$ with corresponding eigenvalues $\\lambda_1^t,\\ldots,\\lambda_n^t$.\n",
    "\n",
    "**Proof**. Note that since $M$ is symmetric, it has a complete orthonormal eigen-basis, and can be diagonalized into $M = U\\Sigma U^T$ with columns of $U$ being the eigenvectors, and diagonal entries of $\\Sigma$ being the eigen-values. And we also know that $U^T=U^{-1}$. \n",
    "\n",
    "Then $M^t = \\left(U\\Sigma U^T\\right)^t = \\left(U\\Sigma U^{-1}\\right)^t =(U\\Sigma U^{-1})(U\\Sigma U^{-1})\\ldots (U\\Sigma U^{-1}) =U\\Sigma^tU^{-1}=U\\Sigma^t U^T$, \n",
    "\n",
    "which is readily seen to be the diagonalization of $M^t$, because $\\Sigma^t$ is diagonal. So the columns of $U$ are the eigen-vectors of $M^t$, and the diagonal values of $\\Sigma^t$ (given by $\\lambda_1^t,\\dots,\\lambda_n^t$) -- the corresponding eigen-values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that when thinking about the computation of \n",
    "the stationary distribution (e.g., via the power method) we are interested in how well $\\mathbf{x}M^t$ \n",
    "approximates the stationary probability distribution $\\pi$. In other words, we are interested in the \n",
    "value of $||\\mathbf{x}M^t - \\pi||_2$. Given that $\\mathbf{x}$ is a linear combination \n",
    "of the eigenvectors, i.e., $\\mathbf{x} = \\sum_{i\\leq n}\\alpha_i \\mathbf{e_i}$, with $\\alpha_i = \\mathbf{x}\\mathbf{e_i}^T$ (careful, this could be negative!), show that  $||\\mathbf{x}M^t - \\pi||_2 \\leq \\sqrt{n}\\lambda_2^t$. Don't forget that  $||\\mathbf{y}||_2 = (\\mathbf{y}\\mathbf{y}^T)^{1/2}$ for any row vector $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//scratch cell\n",
    "\n",
    "$e_iM^t = \\lambda_i^t e_i$\n",
    "\n",
    "we need to show that:\n",
    "\n",
    "$\\alpha_1e_1 = \\frac{1}{\\sqrt{n}}\\sqrt{n}\\pi$\n",
    "\n",
    "$||e_1||_2=1$\n",
    "\n",
    "$e_1(i) = q $ \n",
    "\n",
    "$\\sqrt{n q^2} = 1\\rightarrow nq^2=1\\rightarrow q^2 = \\frac{1}{n}\\rightarrow q =\\frac{1}{\\sqrt{n}} $\n",
    "\n",
    "$e_1(i)=\\frac{1}{\\sqrt{n}}$ for every $i$\n",
    "\n",
    "$\\pi(i)=\\frac{1}{n}$ for every $i$\n",
    "\n",
    "From the last two equations we get\n",
    "\n",
    "$e_1 = \\sqrt{n}\\pi$\n",
    "\n",
    "Now we only need to show that $\\alpha_1 = \\frac{1}{\\sqrt{n}}$\n",
    "\n",
    "$\\alpha_1 = xe_1^T = \\sum_{i=1}^nx(i)e_1(i)=\\sum_{i=1}^nx(i)\\frac{1}{\\sqrt{n}}=\\frac{1}{\\sqrt{n}}\\sum_{i=1}^nx(i)=\\frac{1}{\\sqrt{n}}$\n",
    "\n",
    "$||x||_2 = \\sqrt{x(1)^2+x(2)^2+\\ldots + x(n)^2}$\n",
    "\n",
    "Now I know that $x(1)+x(2)+\\ldots+x(n)=1$\n",
    "\n",
    "Since $x(i)\\leq 1$ for every $i$ then,\n",
    "\n",
    "$x(1)^2+x(2)^2+\\ldots +x(n)^2\\leq x(1)+x(2)+\\ldots+x(n)=||x||_1$\n",
    "\n",
    "this means that\n",
    "\n",
    "$||x||_2\\leq \\sqrt{||x||_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " First, note that $xM^t = \\sum_{i\\leq n}\\alpha_i e_i M^t = \\sum_{i\\leq n}\\alpha_i \\lambda_i^t e_i$, since $e_i$ is an eigen-vector of $M^t$ by the first part. Now let's show that $xM^t - \\pi = \\sum_{2\\leq i\\leq n}\\alpha_i \\lambda_i^t e_i$. \n",
    "\\begin{align*}\n",
    "    xM^t - \\pi\n",
    "    &= \\sum_{i\\leq n}\\alpha_i \\lambda_i^t e_i - \\pi = \\sum_{2\\leq i\\leq n}\\alpha_i \\lambda_i^t e_i + \\alpha_1 e_1 - \\pi\\text{, because }\\lambda_1 = 1 \\\\\n",
    "    &= \\sum_{2\\leq i\\leq n}\\alpha_i \\lambda_i^t e_i + \\frac{1}{\\sqrt{n}}\\sqrt{n}\\pi-\\pi\\\\\n",
    "    &= \\sum_{2\\leq i\\leq n}\\alpha_i \\lambda_i^t e_i,\\text{ because $\\alpha_1 = <x,e_1> = \\frac{1}{\\sqrt{n}}$ and $e_1= \\sqrt{n}\\pi$ by direct computation}\n",
    "\\end{align*}\n",
    "So we want to show $||\\sum_{2\\leq i\\leq n}\\alpha_i \\lambda_i^t e_i||_2 \\leq\\sqrt{n}\\lambda_2^t$. But first let's show that $\\alpha_i^2\\leq 1$:\n",
    "\\begin{align*}\n",
    "    \\alpha_i^2 \n",
    "    &= (<x,e_i>)^2 \\leq ||x||_2||e_i||_2 \\text{ by the Cauchy-Schwarz inequality}\\\\\n",
    "    &= ||x||_2\\text{, because $||e_i||_2=1$}\\\\ \n",
    "    &\\leq \\sqrt{||x||_1}  = 1 \\text{, because entries of $x$ are less than 1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to finish the proof.\n",
    "\\begin{align*}\n",
    "    ||\\sum_{2\\leq i\\leq n}\\alpha_i \\lambda_i^t e_i||_2^2\n",
    "    &= <\\sum_{2\\leq i\\leq n}\\alpha_i \\lambda_i^t e_i,\\sum_{2\\leq j\\leq n}\\alpha_j \\lambda_j^t e_j> \\\\\n",
    "    &= \\sum_{2\\leq i\\leq n}\\alpha_i \\lambda_i^t <e_i,\\sum_{2\\leq j\\leq n}\\alpha_j \\lambda_j^t e_j>\\text{ by linearity of inner product in the first coordinate}\\\\\n",
    "    &= \\sum_{2\\leq i\\leq n}\\alpha_i \\lambda_i^t \\left[\\sum_{2\\leq j\\leq n}\\alpha_j \\lambda_j^t <e_i,e_j>\\right]\\\\\n",
    "    &= \\sum_{2\\leq i\\leq n}\\alpha_i \\lambda_i^t \\alpha_i\\lambda_i^t \\text{, since $<e_i,e_j> = 0$ if $i\\neq j$, and $<e_i,e_j> = 1$ if $i=j$}\\\\\n",
    "    &=\\sum_{2\\leq i\\leq n}\\alpha_i^2 \\lambda_i^{2t} \\leq \\sum_{2\\leq i\\leq n} \\lambda_i^{2t}\\text{, since $\\alpha_i^2 \\leq 1$ by the computation above} \\\\\n",
    "    &\\leq (n-1)\\lambda_2^{2t}\\leq n\\lambda_2^{2t}\\text{ because $\\lambda_2$ is larger than all other eigen-values, except $\\lambda_1$}.\n",
    "\\end{align*}\n",
    "Thus, $||\\sum_{2\\leq i\\leq n}\\alpha_i \\lambda_i^t e_i||_2^2 \\leq n\\lambda_2^{2t} \\implies ||\\sum_{2\\leq i\\leq n}\\alpha_i \\lambda_i^t e_i||_2 \\leq \\sqrt{n}\\lambda_2^t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mixing time\n",
    "\n",
    "**Definition:** The mixing time of an ergodic Markov chain with transiton matrix $M$ is $t$ if for every starting\n",
    "distribution $\\mathbf{x}$, the distribution $\\mathbf{x} M^t$ \n",
    "satisfies $\\|\\mathbf{x}M^t-\\pi\\|_1\\leq 1/4$.\n",
    "Where, $||_1$\n",
    "denotes the 1-norm and the constant “1/4” is arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What does this mean for the proof we did mean for the mixing time?\n",
    "\n",
    "- First we remind ourselves that for any vector $\\mathbf{p}$, $|\\mathbf{p}|_1\\leq 1\\sqrt{n} |\\mathbf{p}|_2$.\n",
    "\n",
    "- Thus: $\\|\\mathbf{x}M-\\pi\\|_1\\leq 1\\sqrt{n} |\\mathbf{x}M-\\pi\\|_2\\leq \\sqrt{n}\\lambda_\\max^t$ \n",
    "\n",
    "- And $\\sqrt{n}\\lambda_\\max^t\\leq 1/4$ means that $t=O\\left(\\frac{\\log n}{\\lambda_\\max}\\right)$\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
